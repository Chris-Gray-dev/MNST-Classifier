{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CISC_452_Christopher_Gray_10185372_14cmg5.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cUWXglNPMeLJ",
        "colab_type": "text"
      },
      "source": [
        "**CISC 452 Assignment 2**\n",
        "\n",
        "**Chris Gray**\n",
        "\n",
        "**10185372**\n",
        "\n",
        "**14cmg5**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T1Cm8Bt6BrLR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install tensorflow-gpu==2.0.0rc numpy --no-cache-dir"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rc7zfrLKCMC0",
        "colab_type": "code",
        "outputId": "3fb30ed9-e4bf-41cd-eb9f-45e2a0d0f877",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "\n",
        "from tensorflow.keras.datasets import mnist\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import time\n",
        "import random\n",
        "import os\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from math import exp\n",
        "\n",
        "class Model():\n",
        "  \n",
        "  def __init__(self, Layers, LearningRate = 5.0, Epochs = 100, BatchSize = 32, MomentiumFactor = 0.75, MSE_Threshold = 0.250):\n",
        "    self.NumLayers = len(Layers)\n",
        "    self.Layers = Layers\n",
        "    self.Weights =self.__Gen_Weights() \n",
        "    self.Biases = self.__Gen_Biases() \n",
        "    self.LearningRate = LearningRate\n",
        "    self.Epochs = Epochs\n",
        "    self.BatchSize = BatchSize\n",
        "    self.MomentiumFactor = MomentiumFactor\n",
        "    self.MSE_Threshold = MSE_Threshold\n",
        "    self.__MSE = 999 # set starting MSE to a large number at first as it wont get updated until epoch 10\n",
        "    self.__mWeights = self.__Gen_Empty_Weights() # stores the last deltaW to be used in momentium \n",
        "    self.__mBiases = self.__Gen_Empty_Biases() # stores the last deltaB to be used in momentium \n",
        "  \n",
        "  # Takes in outputs and returns the outputs of the output nodes\n",
        "  def __call__(self,inputs):\n",
        "    activation = inputs\n",
        "    for weight, bias in zip(self.Weights, self.Biases):\n",
        "      activation = np.dot(weight,activation) + bias # y = wx+b\n",
        "      activation = sigmoid(activation) # apply the activation function\n",
        "    return activation\n",
        "  \n",
        "  # Public Functions\n",
        "  \n",
        "  # trains our network on training data in batches\n",
        "  def Train(self,TrainingData):\n",
        "    random.shuffle(TrainingData) # get an even distribution of data points\n",
        "    batches = self.__Gen_Batches(TrainingData) # create batches on the shuffled data\n",
        "    for epoch in range(1,(self.Epochs+1)): \n",
        "      for batch in batches:\n",
        "        self.__Batch_Train(batch) # train on our batches \n",
        "      if epoch % 10 == 0:\n",
        "        # Decrase learning rate and momentium factor as time goes on \n",
        "        self.LearningRate *=0.9\n",
        "        self.MomentiumFactor *=0.9\n",
        "        self.__Print_Epoch_Metrics(epoch,TrainingData) # print out metrics\n",
        "        \n",
        "      # check to see if this epoch we reach our MSE goal, if so stop training\n",
        "      if self.__MSE <= self.MSE_Threshold:\n",
        "        break\n",
        "\n",
        "  # Evaluate the network on data and return relevant metrics\n",
        "  def Evaluate_Data(self, data):\n",
        "    correct = 0 \n",
        "    mse = 0\n",
        "    ds = []\n",
        "    ys = []\n",
        "    for x , d in data:\n",
        "      d = np.argmax(d)\n",
        "      ds.append(d)\n",
        "      \n",
        "      y = np.argmax(self(x))\n",
        "      ys.append(y)\n",
        "      \n",
        "      mse += Mean_Squared_Error(d,y)\n",
        "      if d == y:\n",
        "        correct += 1\n",
        "        \n",
        "    acc = correct / len(data) * 100\n",
        "    mse = mse / len(data) # divide total MSE by number of tests\n",
        "    return correct , acc , mse , ds , ys\n",
        "      \n",
        "  # Private Functions\n",
        "  \n",
        "  # Derivative of MSE:((d-y)^2)\n",
        "  def __Error(self, desired, actual):\n",
        "    return desired - actual\n",
        "  \n",
        "  # Train the network on a batch of inputs and labels\n",
        "  def __Batch_Train(self, batch):\n",
        "    delta_Ws = self.__Gen_Empty_Weights()\n",
        "    delta_Bs = self.__Gen_Empty_Biases()\n",
        "    \n",
        "    for inputs, label in batch:\n",
        "      dWs , dBs = self.__Back_Propagation(inputs,label)\n",
        "      # sum the delta from each data point in the batch\n",
        "      for i in range(len(delta_Ws)):\n",
        "        delta_Ws[i] += dWs[i] \n",
        "      for i in range(len(delta_Bs)):\n",
        "        delta_Bs[i] += dBs[i]\n",
        "        \n",
        "    for i in range(len(self.Weights)):\n",
        "      deltaW = (self.LearningRate / len(batch)) * delta_Ws[i] # divide by batch size to get the mean of the deltas\n",
        "      self.Weights[i] += deltaW + (self.MomentiumFactor * self.__mWeights[i]) # apply delta plus momentium\n",
        "      self.__mWeights[i] = deltaW # save the delta as momentium for the next change\n",
        "      \n",
        "    for i in range(len(self.Biases)):\n",
        "      deltaB = (self.LearningRate / len(batch)) * delta_Bs[i] # divide by batch size to get the mean of the deltas   \n",
        "      self.Biases[i] += deltaB + (self.MomentiumFactor * self.__mBiases[i]) # apply delta plus momentium\n",
        "      self.__mBiases[i] = deltaB # save the delta as momentium for the next change\n",
        "  \n",
        "  # Calcualte the changes of weights\n",
        "  def __Back_Propagation(self, inputs,label):\n",
        "    delta_Ws = self.__Gen_Empty_Weights()\n",
        "    delta_Bs = self.__Gen_Empty_Biases()\n",
        "    \n",
        "    activations = [] # raw activations before function has been applied: a\n",
        "    function_activations = [inputs] # activations after the output function has been applied: f(a)\n",
        "    \n",
        "    for weight,bias in zip(self.Weights, self.Biases):\n",
        "      net = np.dot(weight,inputs) + bias #  y = mx+b\n",
        "      activations.append(net) # save this activation before the simoid is applied\n",
        "      \n",
        "      fNet = sigmoid(net) # apply the sigmoid to get the output(y) of the layer\n",
        "      function_activations.append(fNet) # save this value\n",
        "      inputs = fNet\n",
        "    \n",
        "    outputLayer = -1 # Start by calculating the error on the ouput layer\n",
        "    \n",
        "    actual = function_activations[outputLayer] # last f(a) is our output\n",
        "    error = label - actual # d - y\n",
        "    \n",
        "    fPrimeOutput = sigmoid_prime(activations[outputLayer]) # f'(a) of the output layer\n",
        "    delta = error * fPrimeOutput\n",
        "    \n",
        "    delta_Bs[outputLayer] = delta\n",
        "    # calculate the change in weights for each weight going to the output layer\n",
        "    delta_Ws[outputLayer] = np.dot(delta,function_activations[outputLayer -1].transpose()) \n",
        "    \n",
        "    for i in range(2,self.NumLayers):\n",
        "      layer = -i # work backwords from the last hidden layer until the input layer\n",
        "      a = activations[layer] # get the activation of the layer we are working on\n",
        "      fPrimeOfLayer = sigmoid_prime(a) # get the f'(a) of the layer we are working on\n",
        "      \n",
        "      delta = np.dot(self.Weights[layer + 1].transpose(),delta) * fPrimeOfLayer # update the delta based on the sum of error from the previous layer \n",
        "      \n",
        "      delta_Bs[layer] = delta\n",
        "      # calculate the change in weights for each weight going to this layer\n",
        "      delta_Ws[layer] = np.dot(delta,function_activations[layer-1].transpose()) \n",
        "      \n",
        "    return delta_Ws, delta_Bs\n",
        "  \n",
        "  # Print out metrics for current network\n",
        "  def __Print_Epoch_Metrics(self, epoch,TrainingData):\n",
        "    print(\"Epoch #%d\" %(epoch))\n",
        "    c, a,mse,ds,ys= self.Evaluate_Data(TrainingData)\n",
        "    self.__MSE = mse # update MSE\n",
        "    print(\"Accuracy: %d/%d | %2.2f%% | MSE: %2.4f\" %(c,len(TrainingData),a,mse))  \n",
        "  \n",
        "  # splits the data into batches of the model's batch size\n",
        "  def __Gen_Batches(self, data):\n",
        "    batches = []\n",
        "    for i in range(0,len(data),self.BatchSize):\n",
        "      batches.append(data[i:i+self.BatchSize])\n",
        "    return batches\n",
        "  \n",
        "  # Generates a list of weight matrices based on the number of nodes in each layer\n",
        "  def __Gen_Weights(self):\n",
        "    weights = []\n",
        "    for inputDimension,outputDimension in zip(self.Layers[:-1],self.Layers[1:]):\n",
        "      weights.append(np.random.randn(outputDimension,inputDimension))\n",
        "    return weights\n",
        "  \n",
        "   # Generates a list of bias vectors based on the number of nodes in each layer\n",
        "  def __Gen_Biases(self):\n",
        "    biases = []\n",
        "    for nodes in self.Layers[1:]:\n",
        "      biases.append(np.random.randn(nodes,1))\n",
        "    return biases\n",
        "  \n",
        "  # Generates a list of the weight matrices filled with 0s\n",
        "  def __Gen_Empty_Weights(self):\n",
        "    weights = []\n",
        "    for w in self.Weights:\n",
        "      weights.append(np.zeros(w.shape))\n",
        "    return weights\n",
        "  \n",
        "  # Generates a list of the bias vectors filled with 0s\n",
        "  def __Gen_Empty_Biases(self):\n",
        "    biases = []\n",
        "    for b in self.Biases:\n",
        "      biases.append(np.zeros(b.shape))\n",
        "    return biases\n",
        "  \n",
        "  \n",
        "# sigmoid activation function\n",
        "def sigmoid(x):\n",
        "  return 1.0/(1.0+np.exp(-x))\n",
        "\n",
        "# derivative of the sigmoid function\n",
        "def sigmoid_prime(x):\n",
        "  return sigmoid(x)*(1-sigmoid(x))\n",
        "\n",
        "# calculates the mean squared error of a list of outputs.\n",
        "def Mean_Squared_Error(d,y):\n",
        "  return np.mean((d - y)**2)\n",
        "    \n",
        "# Put all the data in the range of 0 and 1\n",
        "# this ensures all the activations are small\n",
        "# so that larged signed values dont have more sway on the results\n",
        "def Normilize_Data(data, maxValue = 255):\n",
        "  data = np.array(data,dtype=float)\n",
        "  for i in range(len(data)):\n",
        "      data[i] = (data[i] / maxValue)\n",
        "  return data\n",
        "  \n",
        "# Flattens the data into a 1d array , in this case 784,1\n",
        "def Flatten_Data(data):\n",
        "  flattenedData = []\n",
        "  for d in data:\n",
        "    d = np.reshape(d,(d.size,1))\n",
        "    flattenedData.append(d)\n",
        "  return flattenedData\n",
        "\n",
        "# Hot encodes the labels\n",
        "# eg 0's become [1,0,0,0,0,0,0,0,0,0] etc.\n",
        "def Hot_Encode(data):\n",
        "  encodedData = []\n",
        "  for d in data:\n",
        "    encoded = np.zeros((10,1))\n",
        "    encoded[d] = 1\n",
        "    encodedData.append(encoded)\n",
        "  return encodedData\n",
        "\n",
        "def Get_Data():\n",
        "  # Get the data\n",
        "  (X_train,Y_train),(X_test,Y_test) = mnist.load_data()\n",
        "  \n",
        "  # Reformat the data\n",
        "  X_train = Normilize_Data(Flatten_Data(X_train))\n",
        "  X_test = Normilize_Data(Flatten_Data(X_test))\n",
        "  \n",
        "  # Hode encodes the output\n",
        "  Y_train = Hot_Encode(Y_train)\n",
        "  Y_test = Hot_Encode(Y_test)\n",
        "  \n",
        "  # Zip the data\n",
        "  TrainingData = list(zip(X_train, Y_train))\n",
        "  TestingData = list(zip(X_test, Y_test))\n",
        "  return TrainingData, TestingData\n",
        "\n",
        "# Prints the confusion matrix for the data\n",
        "def Print_Confusion_Matrix(ds,ys):\n",
        "  confusionMatrix = confusion_matrix(ds,ys)\n",
        "  cm = pd.DataFrame(confusionMatrix)\n",
        "  print(cm)\n",
        "\n",
        "# Prints classifcation report for the data\n",
        "# includes the precision and recall values among other \n",
        "def Print_Classification_Report(ds,ys):\n",
        "  names = [\"Zero\",\"One\",\"Two\",\"Three\",\"Four\",\n",
        "             \"Five\",\"Six\",\"Seven\",\"Eight\",\"Nine\"]\n",
        "  print(classification_report(ds, ys,target_names = names)) \n",
        "\n",
        "  \n",
        "def main():\n",
        "  TrainingData,TestingData = Get_Data()\n",
        "  # Check that the data is the correct chape\n",
        "  print(\"Data Shapes\")\n",
        "  print(\"X shape:\",TrainingData[0][0].shape)\n",
        "  print(\"Y shape:\",TrainingData[0][1].shape)\n",
        "  \n",
        "  \n",
        "  # Create the model\n",
        "  layers = [784,30,15,10]\n",
        "  LearningRate = 5\n",
        "  BatchSize = 32\n",
        "  Epochs = 150\n",
        "  MomentiumFactor = 0.8\n",
        "  MSE_Threshold = 0.25  \n",
        "  \n",
        "  # Create the model\n",
        "  model = Model(Layers=layers, LearningRate=LearningRate, BatchSize=BatchSize, Epochs=Epochs ,MomentiumFactor=MomentiumFactor, MSE_Threshold=MSE_Threshold)    \n",
        "    \n",
        "  print(\"-----------------------------------------------\")\n",
        "  \n",
        "  \n",
        "  print(\"Beginning to train the network...\")\n",
        "  start = time.time()\n",
        "  # Train the model\n",
        "  model.Train(TrainingData)\n",
        "  end = time.time()\n",
        "  print(\"Training Complete\")\n",
        "  print(\"Training time: %ds\"%((end-start)))\n",
        "  \n",
        "  \n",
        "  print(\"-----------------------------------------------\")\n",
        "  \n",
        "  \n",
        "  print(\"Evaluating Training Data\")\n",
        "  correct , acc , mse , ds , ys = model.Evaluate_Data(TrainingData)\n",
        "  print(\"Accuracy: %d/%d | %2.2f%% | MSE: %2.4f\" %(correct,len(TrainingData),acc,mse)) \n",
        "  Print_Confusion_Matrix(ds,ys)\n",
        "  Print_Classification_Report(ds,ys)\n",
        "  \n",
        "  print(\"-----------------------------------------------\")\n",
        "  \n",
        "  \n",
        "  print(\"Evaluating Testing Data\")\n",
        "  correct , acc , mse , ds , ys = model.Evaluate_Data(TestingData)\n",
        "  print(\"Accuracy: %d/%d | %2.2f%% | MSE: %2.4f\" %(correct,len(TestingData),acc,mse))\n",
        "  Print_Confusion_Matrix(ds,ys)\n",
        "  Print_Classification_Report(ds,ys)\n",
        "  \n",
        "\n",
        "main()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Data Shapes\n",
            "X shape: (784, 1)\n",
            "Y shape: (10, 1)\n",
            "\n",
            "Layer:  1\n",
            "Weights:  (30, 784)\n",
            "Bias:  (30, 1)\n",
            "Layer:  2\n",
            "Weights:  (15, 30)\n",
            "Bias:  (15, 1)\n",
            "Layer:  3\n",
            "Weights:  (10, 15)\n",
            "Bias:  (10, 1)\n",
            "-----------------------------------------------\n",
            "Beginning to train the network...\n",
            "Epoch #10\n",
            "Accuracy: 57248/60000 | 95.41% | MSE: 0.7843\n",
            "Epoch #20\n",
            "Accuracy: 57466/60000 | 95.78% | MSE: 0.7987\n",
            "Epoch #30\n",
            "Accuracy: 58065/60000 | 96.78% | MSE: 0.5694\n",
            "Epoch #40\n",
            "Accuracy: 58340/60000 | 97.23% | MSE: 0.4985\n",
            "Epoch #50\n",
            "Accuracy: 58825/60000 | 98.04% | MSE: 0.3424\n",
            "Epoch #60\n",
            "Accuracy: 58925/60000 | 98.21% | MSE: 0.3237\n",
            "Epoch #70\n",
            "Accuracy: 58990/60000 | 98.32% | MSE: 0.3056\n",
            "Epoch #80\n",
            "Accuracy: 59070/60000 | 98.45% | MSE: 0.2846\n",
            "Epoch #90\n",
            "Accuracy: 59108/60000 | 98.51% | MSE: 0.2776\n",
            "Epoch #100\n",
            "Accuracy: 59142/60000 | 98.57% | MSE: 0.2685\n",
            "Epoch #110\n",
            "Accuracy: 59165/60000 | 98.61% | MSE: 0.2586\n",
            "Epoch #120\n",
            "Accuracy: 59176/60000 | 98.63% | MSE: 0.2544\n",
            "Epoch #130\n",
            "Accuracy: 59185/60000 | 98.64% | MSE: 0.2507\n",
            "Epoch #140\n",
            "Accuracy: 59198/60000 | 98.66% | MSE: 0.2501\n",
            "Epoch #150\n",
            "Accuracy: 59205/60000 | 98.67% | MSE: 0.2455\n",
            "Training Complete\n",
            "Training time: 1954s\n",
            "-----------------------------------------------\n",
            "Evaluating Training Data\n",
            "Accuracy: 59205/60000 | 98.67% | MSE: 0.2455\n",
            "      0     1     2     3     4     5     6     7     8     9\n",
            "0  5895     0     1     6     6     2     4     1     5     3\n",
            "1     1  6684    18     8     6     1     3     5    14     2\n",
            "2    14     2  5839    18    14    14     9    22    21     5\n",
            "3     5     4    20  5995     2    32     7    15    33    18\n",
            "4    10     3     5     0  5780     1    13     3     7    20\n",
            "5    11     1     8    17     8  5337    10     2    16    11\n",
            "6    19     0     4     0     9     9  5872     0     5     0\n",
            "7     3     5    18    10    12     0     4  6196     6    11\n",
            "8     6     8    16    10     3     5     7     2  5784    10\n",
            "9    13     2     8    17    32    14     0    22    18  5823\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "        Zero       0.99      1.00      0.99      5923\n",
            "         One       1.00      0.99      0.99      6742\n",
            "         Two       0.98      0.98      0.98      5958\n",
            "       Three       0.99      0.98      0.98      6131\n",
            "        Four       0.98      0.99      0.99      5842\n",
            "        Five       0.99      0.98      0.99      5421\n",
            "         Six       0.99      0.99      0.99      5918\n",
            "       Seven       0.99      0.99      0.99      6265\n",
            "       Eight       0.98      0.99      0.98      5851\n",
            "        Nine       0.99      0.98      0.98      5949\n",
            "\n",
            "    accuracy                           0.99     60000\n",
            "   macro avg       0.99      0.99      0.99     60000\n",
            "weighted avg       0.99      0.99      0.99     60000\n",
            "\n",
            "-----------------------------------------------\n",
            "Evaluating Testing Data\n",
            "Accuracy: 9541/10000 | 95.41% | MSE: 0.7919\n",
            "     0     1    2    3    4    5    6    7    8    9\n",
            "0  958     0    4    3    0    4    6    2    2    1\n",
            "1    0  1119    1    3    0    1    2    4    5    0\n",
            "2    7     5  978   10    6    2    6    9    9    0\n",
            "3    4     3    4  964    2   16    0    6    6    5\n",
            "4    1     1    2    2  940    2    8    4    3   19\n",
            "5    8     0    2   21    5  824   10    4   12    6\n",
            "6   11     2    4    2    7    4  924    0    4    0\n",
            "7    2     9   18    8    1    0    2  976    3    9\n",
            "8    6     3    5   13    4   16    8    8  907    4\n",
            "9    4     4    0    4   19   12    1    6    8  951\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "        Zero       0.96      0.98      0.97       980\n",
            "         One       0.98      0.99      0.98      1135\n",
            "         Two       0.96      0.95      0.95      1032\n",
            "       Three       0.94      0.95      0.95      1010\n",
            "        Four       0.96      0.96      0.96       982\n",
            "        Five       0.94      0.92      0.93       892\n",
            "         Six       0.96      0.96      0.96       958\n",
            "       Seven       0.96      0.95      0.95      1028\n",
            "       Eight       0.95      0.93      0.94       974\n",
            "        Nine       0.96      0.94      0.95      1009\n",
            "\n",
            "    accuracy                           0.95     10000\n",
            "   macro avg       0.95      0.95      0.95     10000\n",
            "weighted avg       0.95      0.95      0.95     10000\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}